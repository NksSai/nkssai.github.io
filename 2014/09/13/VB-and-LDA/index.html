<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" >
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"/>
  <title>变分推断与LDA | 今天拒绝负能量</title>
  <meta name="description" content="" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" type="text/css" href="/css/component.css" />
  <link rel="stylesheet" type="text/css" href="/css/screen.css" />
  <meta name="generator" content="今天拒绝负能量">
  <script src="http://static.duoshuo.com/embed.js"></script>
  
  
  

  

</head>
<body>
<div class="container">
    <div class="mp-pusher" id="mp-pusher">
        <i id="scroll-up" class="fa fa-angle-up"></i>
        <nav id="mp-menu" class="mp-menu">
            <div class="mp-level">
                <a data-pjax class="back-home" style="font-size: 20px" href="/"><h2 ><i class="fa fa-home"></i>
                        Home</h2></a>
                <ul class="first-level">
                    <li>
                        <a class="fa fa-archive" href="#"><i class="fa fa-angle-left">
                            </i>&nbsp;&nbsp;归档</a>
                        <div class="mp-level page-list">
                            <h2 ><i class="fa fa-archive"></i>
                                归档</h2>
                            <a class="mp-back" href="#">back</a>
                            <form id="search-form" action="">
                                <input type="text" class="search search-archive" placeholder="Search.."/>
                            </form>
                            <ul>
                                <div class="mp-scroll">
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2014/09/30/Gibbs-Sampling-and-B-LDA/">Gibbs Sampling与B-LDA</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2014/09/13/VB-and-LDA/">变分推断与LDA</a>
                                </li>
                                
                                <li class="search-archive-li mp-pjax">
                                    <a href="/2014/09/03/EM-and-GLAD/">EM与GLAD</a>
                                </li>
                                
                                </div>
                            </ul>
                        </div>
                    </li>
                    <li>
                        <a class="fa fa-copy" href="#"><i class="fa fa-angle-left">
                            </i>&nbsp;&nbsp;分类</a>

                        <div class="mp-level page-list">
                            <h2 ><i class="fa fa-copy"></i>
                                分类</h2>
                            <a class="mp-back" href="#">back</a>
                            <form id="search-form" action="">
                                <input type="text" class="search search-category" placeholder="Search.."/>
                            </form>
                            <ul>
                                <div class="mp-scroll">
                                
                                <li class="search-category-li mp-pjax">
                                    <a href="/categories/正经笔记/">&nbsp;&nbsp;&nbsp;正经笔记</a>
                                    <small>2</small>
                                </li>
                                
                                </div>
                            </ul>
                        </div>
                    </li>
                    <li>
                        <a class="fa fa-tags" href="#"><i class="fa fa-angle-left">
                            </i>&nbsp;&nbsp;标签</a>
                        <div class="mp-level page-list">
                            <h2 ><i class="fa fa-tags"></i>
                                标签</h2>
                            <a class="mp-back" href="#">back</a>
                            <form id="search-form" action="">
                                <input type="text" class="search search-tag" placeholder="Search.."/>
                            </form>
                            <ul>
                                <div class="mp-scroll">
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Crowdsourcing/">&nbsp;&nbsp;&nbsp;Crowdsourcing</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/EM/">&nbsp;&nbsp;&nbsp;EM</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/GLAD/">&nbsp;&nbsp;&nbsp;GLAD</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/LDA/">&nbsp;&nbsp;&nbsp;LDA</a>
                                    <small>2</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/MCMC/">&nbsp;&nbsp;&nbsp;MCMC</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/PGM/">&nbsp;&nbsp;&nbsp;PGM</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Sampling/">&nbsp;&nbsp;&nbsp;Sampling</a>
                                    <small>1</small>
                                </li>
                                
                                <li class="search-tag-li mp-pjax">
                                    <a href="/tags/Variational-Bayes/">&nbsp;&nbsp;&nbsp;Variational-Bayes</a>
                                    <small>1</small>
                                </li>
                                
                                </div>
                            </ul>
                        </div>
                    </li>
                    
                    <li class="mp-pjax"><a class="fa fa-user" href="/about">&nbsp;&nbsp;&nbsp;About me</a></li>
                    <li><a class="fa fa-github" href="#">&nbsp;&nbsp;&nbsp;Github</a></li>

                </ul>

            </div>
        </nav>
        <div id="pjax">
            <div class="pjax-hidden" style="display: none">
                
                    <a  data-pjax href="/2014/09/30/Gibbs-Sampling-and-B-LDA/">Gibbs Sampling与B-LDA</a>
                
                    <a  data-pjax href="/2014/09/13/VB-and-LDA/">变分推断与LDA</a>
                
                    <a  data-pjax href="/2014/09/03/EM-and-GLAD/">EM与GLAD</a>
                
                
                    <a data-pjax href="/categories/正经笔记/">&nbsp;&nbsp;正经笔记</a>
                
                
                    <a data-pjax href="/tags/Crowdsourcing/">&nbsp;&nbsp;Crowdsourcing</a>
                
                    <a data-pjax href="/tags/EM/">&nbsp;&nbsp;EM</a>
                
                    <a data-pjax href="/tags/GLAD/">&nbsp;&nbsp;GLAD</a>
                
                    <a data-pjax href="/tags/LDA/">&nbsp;&nbsp;LDA</a>
                
                    <a data-pjax href="/tags/MCMC/">&nbsp;&nbsp;MCMC</a>
                
                    <a data-pjax href="/tags/PGM/">&nbsp;&nbsp;PGM</a>
                
                    <a data-pjax href="/tags/Sampling/">&nbsp;&nbsp;Sampling</a>
                
                    <a data-pjax href="/tags/Variational-Bayes/">&nbsp;&nbsp;Variational-Bayes</a>
                
                <a data-pjax class="fa fa-user" href="/about">&nbsp;&nbsp;&nbsp;About me</a>
            </div>
            <nav class="nexus">
                <li  style="border-left: 1px solid #c6d0da;">
                    <a id="trigger" href="#"><i class="fa fa-bars"></i></a>
                </li>
                <li ><a id="nexus-back" data-pjax href="/">今天拒绝负能量</a></li>
                
                <div id="nav-container">
                    <div class="post-navbar" style="line-height: 63px;display:none">
                        <li id="navbar-title"><a href="#">变分推断与LDA</a></li>
                        <li id="navbar-toc" style="border-left: none">
                            <a style="padding-right: 15px">
                                <span id="toc-content" >Introduction</span><i class="fa fa-chevron-down" ></i>
                            </a>
                            <div class="hidden-box">
                                <ul id="toc"></ul>
                            </div>
                        </li>
                    </div>
                </div>
                
            </nav>

            <div class="scroller">
            <div class="scroller-inner">


<!-- -->
<!--<body class="post-template">-->
<!---->
  

<main class="content" role="main">
    <article class="post" >
    <span class="post-meta">
                  <div class="tag-tile">
                      
                      
                      <a data-pjax href='/tags/LDA/' style='color:#D5D5D5'>LDA</a>
                      
                      <a data-pjax href='/tags/Variational-Bayes/' style='color:#D5D5D5'>Variational-Bayes</a>
                      
                      <a data-pjax href='/tags/PGM/' style='color:#D5D5D5'>PGM</a>
                      
                      
                  </div>
                <h1 class="post-title" style="margin: 14px 0;color:#50585D">变分推断与LDA</h1>

                    <div class="post-meta">
                        Post on<span class="fa fa-clock-o"></span>
                        <time datetime="2014-09-12T16:00:00.000Z"
                              itemprop="datePublished">Sep 13 2014</time>
                    </div>
    </span>

        <section class="post-content">
            <p>首先从LDA（Latent Dirichlet Allocation）开始，LDA是文档聚类的生成模型，又是PGM（概率图模型）很典型的一个例子，先看看它的生成图模型（这里并不讨论LDA与PLSA优劣，或者是其他有关文档主题发现模型有关的问题，主要是以LDA为一个例子，学习变分推断这个方法）。 <a id="more"></a></p>
<div class="figure">
<img src="http://nkssai.qiniudn.com/LDA.png">
</div>
<p>先理解一下这个模型的话，图中每一个圆圈指的是随机变量或者参数，涂黑的圆圈代表的是被观测到的变量，这里的<span class="math">\(w\)</span>就是被观测到的词的分布，包裹着圈的方框，其实代表的是实体，其中的字母代表实体的数目，图中外面的实体就是文章的个数为<span class="math">\(M\)</span>，其中每个文章中单词数为<span class="math">\(N\)</span>，<span class="math">\(z\)</span>为指示生成<span class="math">\(w\)</span>的主题，<span class="math">\(\theta\)</span>为生成主题<span class="math">\(z\)</span>的分布。<span class="math">\(\alpha\)</span>和<span class="math">\(\beta\)</span>都是超参。</p>
<p>所以生成整个文章的过程应该是，共有<span class="math">\(V\)</span>个单词，<span class="math">\(K\)</span>个主题。每个文章中都有隐含的分布<span class="math">\(\theta\)</span>来生成主题（这里应该注意的是一篇文章可能有很多主题,每一次生成单词的时候都会选择主题），选定主题<span class="math">\(z\)</span>后，通过<span class="math">\(z\)</span>指定的<span class="math">\(\beta\)</span>（<span class="math">\(\beta\)</span>应该为一个）中的一行选择单词<span class="math">\(w\)</span>。</p>
<p>形式化来表示为: 选择<span class="math">\(N \sim Poisson(\xi)\)</span><br> 选择<span class="math">\(\theta \sim Dir(\alpha)\)</span><br> 对于每一个单词<br> 选择主题<span class="math">\(z_n \sim Multinomial(\theta)\)</span><br> 由通过<span class="math">\(z\)</span>在<span class="math">\(\beta\)</span>中选择出来的一个多项分布<span class="math">\(p(w_n|z_n,\beta)\)</span>生成单词<span class="math">\(w_n\)</span>。</p>
<p>整个生成过程写出来应该是</p>
<p><span class="math">\[p(\theta,z,w|\alpha,\beta)=p(\theta|\alpha)\prod_{n=1}^Np(z_n|\theta)p(w_n|z_n,\beta) \]</span></p>
<p>对于这个模型推断的关键在于其中对于给定参数以及观测的<span class="math">\(w\)</span>的后验分布<span class="math">\(p(\theta,z|w,\alpha,\beta)=\frac{p(\theta,z,w|\alpha,\beta)}{p(\theta|\alpha,\beta)}\)</span>的计算。由于有两个隐藏变量纠缠在一起，我们没法直接用像EM一样方法准确求解，这里我们就需要变分推断了。</p>
<p>让我们先回到之前的EM的那个式子：</p>
<p><span class="math">\[\ln p(X)=\mathcal{L}(q)+KL(q||p)\]</span></p>
<p>其中</p>
<p><span class="math">\[\mathcal{L}(q)=\int q(Z)\ln \frac{ p(X,Z)}{q(Z)}\mathcal{d}z\]</span></p>
<p><span class="math">\[KL(q||p)=-\int q(Z)\ln \frac{p(Z|X)}{q(Z)}\mathcal{d}z\]</span></p>
<p>两个与之前的式子不太一样，一是将累加变成了积分，这只是对离散与连续变量的不同而已，另一方面是我们没有写<span class="math">\(\theta\)</span>，这是因为在我们要处理的模型中，不只是有隐藏变量<span class="math">\(z\)</span>还有其他的随机变量<span class="math">\(\theta\)</span>,所以这里的<span class="math">\(Z\)</span>就代表所有未知随机变量。我们之前的方法是通过用<span class="math">\(q(z)\)</span>来接近<span class="math">\(p(Z|X)\)</span>来极大化<span class="math">\(\mathcal{L}\)</span>，那时我们采用的是令<span class="math">\(q(z)=p(Z|X,\theta^{old})\)</span>，但是此时我们要优化的量不只是简单的隐藏变量<span class="math">\(z\)</span>，而是整个集合<span class="math">\(\{z_n\}\)</span>，其中<span class="math">\(\{z_n\}\)</span>之间也有复杂的联系，所以我们没法直接优化。</p>
<p>为了解决由于<span class="math">\(\{z_n\}\)</span>之间相互纠结而导致的难以求解的问题，我们索性将将<span class="math">\(q(Z)\)</span>因子化为</p>
<p><span class="math">\[q(Z)=\prod_{i=1}^{M}q_i(Z_i)\]</span></p>
<p>其实就是将不同<span class="math">\(z_i\)</span>之间的联系割断，然后用来接近真实的<span class="math">\(q(Z)\)</span>，（这里要注意的是将<span class="math">\(Z\)</span>分成互不重叠的集合<span class="math">\(\{z_i\}\)</span>，并不一定是一个个的拆分，有时候分成几个团反而更容易计算），这种假设并不是平白无故的产生的，在系统结构是well-mixed，即生成过程是完全图的情况下，根据平均场(Mean Field Method)理论，这种时候往往可以用一个单体来代替周围复杂的相互作用（这点是我自己的理解，可能并不准确，关于平均场的部分在之后应该还会有补充）。所以，对于一个<span class="math">\(z_i\)</span>的状况用一个<span class="math">\(q_i(z_i)\)</span>来描述也是合理的，而且之后我们会见到在LDA中，采用以<span class="math">\(\gamma\)</span>为参数的对应分布来接近<span class="math">\(\theta\)</span>的真实分布。</p>
<p>至此，我们找到了接近真实<span class="math">\(q(Z)\)</span>其实就是<span class="math">\(p(Z|X)\)</span>的方法，剩下的就是推出具体的式子了。这里我见到两种证明方法，一种是将<span class="math">\(\mathcal{L}\)</span>看做是以<span class="math">\(q(Z)\)</span>为参数的泛函，然后再利用变分法以及Euler-Lagrange方程求解其极值，其中将<span class="math">\(q(Z)\)</span>置换为<span class="math">\(q_i(z_i)\)</span>乘积，然后由于存在<span class="math">\(q_i(z_i)\)</span>积分和为1的约束，再利用拉格朗日乘子法就可以求解（本人数学基础为渣，这部分还是看看<a href="http://blog.huajh7.com/wp-content/uploads/2013/03/Variational-Inference-full.pdf" target="_blank" rel="external">大神的介绍</a>，这里也只能感叹各种不同学科背后数学的紧密联系，果然数学才是这个世界的真理）。</p>
<p>另一种是来自《PRML》中的比较容易理解的求解方式. 先考虑极大化<span class="math">\(\mathcal{L}\)</span>的过程，对于其中一个<span class="math">\(q_i(z_i)\)</span>，我们可以将<span class="math">\(\mathcal{L}\)</span>改写为</p>
<p><span class="math">\[\begin{split} \mathcal{L}(q)=&amp; \int \prod_i q_i\ln p(X,Z)dZ-\int \prod_i q_i\sum_i\ln q_i dZ\\
    =&amp;\int \prod_iq_i\bigg\{\ln p(X,Z)-\sum_i\ln q_i\bigg\}dZ \\ \end{split}\]</span></p>
<p>考虑其中的一个<span class="math">\(q_j\)</span>则有</p>
<p><span class="math">\[\begin{split} \mathcal{L}(q) =&amp; \int q_j \bigg\{ \int \ln p(X,Z)\prod_{i \neq j}q_idZ_i\bigg\}dZ_j - \int q_j \ln q_j dZ_j + const \\
= &amp; \int q_j \ln \widetilde{p}(X,Z_j)dZ_j - \int q_j \ln q_j dZ_j + const \\
= &amp; \int q_j \frac{\widetilde{p}(X,Z_j)}{q_j}dZ_j + const\\
= &amp; - KL(q_j || \widetilde{p}(X,Z_j)) + const
\end{split}\]</span></p>
<p>其中</p>
<p><span class="math">\[\begin{split} \ln\widetilde{p}(X,Z_j)=&amp;\int \ln p(X,Z)\prod_{i \neq j}q_idZ_i \\
=&amp; E_{i\neq j}[\ln p(X,Z)]
\end{split}\]</span></p>
<p>这时候，我们就发现在给定所有的<span class="math">\(\{q_{i\neq j}\}\)</span>时，想要极大化<span class="math">\(\mathcal{L}\)</span>等价于极小化<span class="math">\(q_j\)</span>和<span class="math">\(\widetilde{p}(X,Z_j)\)</span>的KL散度，如果我们用<span class="math">\(q^*_j(Z_j)\)</span>表示<span class="math">\(q_j(Z_j)\)</span>的最优值，则那我们只要让<span class="math">\(q_j^*(Z_j)\)</span>为<span class="math">\(E_{i\neq j}[\ln p(X,Z)]\)</span>就可以了。准确写出来应该是：</p>
<p><span class="math">\[q^*_j(Z_j)=\frac{exp(E_{i\neq j}[\ln p(X,Z)]}{\int exp(E_{i\neq j}[\ln p(X,Z)])dZ_j}\]</span></p>
<p>这样我们又得到了之前在EM中M步优化的方法，将纠缠在一起的<span class="math">\(Z\)</span>分割成相互独立的集合<span class="math">\(\{z_j\}\)</span>，然后依次迭代优化，直到收敛。</p>
<p>至此，我们最初的问题好像是解决了。其实在这种基础上还有一些扩展的算法，比如下面这个VBEM算法。</p>
<p>我们之前在讨论模型时，将所有的隐藏变量和参数都放到<span class="math">\(Z\)</span>中，但是实际情况中，隐藏变量和参数往往是不同的，所以我们其实可以采用与EM很相似的过程，将隐藏变量与参数也分开优化，这就是所谓的变分EM算法，（其实所谓的变分法应该就是对于泛函求极值的方法，而对于我们面对的难以求解的模型，将复杂的那部分根据平均场理论拆开，然后就能得到一个个以<span class="math">\(q_i(z_i)\)</span>为参数的泛函，利用变分法求解其极值，所得到的算法都可以成为变分推断。好吧，这句是我胡扯的。）</p>
<p>在VBEM中，要优化的式子与变分推断没什么大的不同,如果用m表示其中的超参的话，我们能发现在用<span class="math">\(\mathcal{L}\)</span>接近似然的时候依然有</p>
<p><span class="math">\[\ln p(X|m) = \mathcal{L}(q_z(Z),q_{\theta}(\theta),X) + KL(q||p)\]</span></p>
<p>所以我们发现，其实VBEM只是把隐藏变量和参数的优化分开，最终EM的迭代过程其实也很相似</p>
<p><span class="math">\[\ln q_z^{(t+1)} \propto  E_{q_{\theta}^{(t)}}[\ln p(X,Z|\theta,m)]\]</span></p>
<p><span class="math">\[q_{\theta}^{(t+1)} \propto p(\theta,m)exp E_{q_{x}^{(t+1)}}(x)\big[\ln p(X,Z|\theta,m)\big]\]</span></p>
<p>这里有一张与EM过程相似的图，解释了相似的过程</p>
<div class="figure">
<img src="http://nkssai.qiniudn.com/VBEM.png" alt="VBEM"><p class="caption">VBEM</p>
</div>
<p>由于我们把所有的未知变量全都拿到<span class="math">\(Z\)</span>和<span class="math">\(\theta\)</span>中去了，所以在我们不更新超参的时候，其实整个模型的似然上界是不变的，VBEM的EM过程是在接近现有的上界的过程，而超参的更新才是极大化上界的过程(这一部分有不少讲的很清楚的资料比如这篇<a href="http://www.cse.buffalo.edu/faculty/mbeal/thesis/beal03_2.pdf" target="_blank" rel="external">介绍</a>)。</p>
<p>不过我们发现一个问题，在不优化超参的时候，上界其实没有变化的，而我们却要通过近似EM的迭代来求一个局部最优解，是不是有一些简单的情况，可以让我们直接就求出来上界的优化方法呢？</p>
<p>我们在选择超参与先验分布的时候往往会选择对应的共轭分布，为的是先验分布于似然函数相乘之后得到的后验依然是相同的分布族。即在很多时候，我们已经知道了后验分布的具体形式，那我们只要假设<span class="math">\(q_j\)</span>依然符合相应的分布，并且给它对应的参数作为相应的充分统计量，就可以免去相对复杂的推导过程，直接通过变分下界的极大化，来求得优化的方法。</p>
<p>下面以一维高斯为例，尝试一下两种解法：</p>
<p>对于一个一维高斯分布，其观测数据为<span class="math">\(X={x_1,...,X_N}\)</span>,我们希望推断的是均值的<span class="math">\(\mu\)</span>和方差的倒数<span class="math">\(\sigma\)</span>,其实就是高斯的充分统计量。</p>
<p>首先先引入共轭分布的:</p>
<p><span class="math">\[p(\mu|\tau)=\mathcal{N}(\mu|\mu_0, (\lambda_0\tau)^{-1})\]</span></p>
<p><span class="math">\[p(\tau)=Gam(\tau|a_0,b_0)\]</span></p>
<p>对于观测到的数据<span class="math">\(X\)</span>有似然为：</p>
<p><span class="math">\[p(X|\mu,\tau)=(\frac{\tau}{2\pi})exp\big\{-\frac{\tau}{2}\sum_{n=1}^{N}(x_n-\mu)^2\big\}\]</span></p>
<p>我们斩断<span class="math">\(\mu\)</span>与<span class="math">\(\tau\)</span>的联系，</p>
<p><span class="math">\[q(\mu,\tau)=q_\mu(\mu)q_\tau(\tau)\]</span></p>
<p>然后对于其中的<span class="math">\(q_\mu(\mu)\)</span>尤其最优解<span class="math">\(q_\mu^*(\mu)\)</span>:</p>
<p><span class="math">\[\begin{split}\ln q^*_u(\mu)&amp;= \mathbb{E}_r[\ln p(\mathbf{X}|\mu,\tau)+\ln p(\mu|\tau)]+const \\&amp;= -\frac{\mathbb{E}[\tau]}{2}\{\lambda_0(\mu-u_0)^2+\sum^N_{n=1}(x_n-\mu)^2\}+const \\\end{split}\]</span></p>
<p>有一些地方要注意，在求<span class="math">\(E[\ln p(X,Z)]\)</span>时，由于我们要利用改进<span class="math">\(\mu\)</span>来优化，所以我们只关心与<span class="math">\(\mu\)</span>有关的部分，表现在生成模型中其实就是只有图中直接相连和有共同子节点的才有关,所以我们在上式的期望中仅有两项。我们发现其实大括号中间是<span class="math">\(\mu\)</span>的二次项，显然<span class="math">\(q_u(u)\)</span>是个高斯分布，所以我们可以求解出来</p>
<p><span class="math">\[\begin{split}u_N &amp;=\frac{\lambda_0u_0+N\bar{x}}{\lambda_0+N} \\\lambda_N &amp;=(\lambda_0+N)\mathbb{E}[\tau] \\\end{split}\]</span></p>
<p>然后对于<span class="math">\(q_\tau(\tau)\)</span>，有：</p>
<p><span class="math">\[\begin{split}\ln q^*_r(\tau)&amp;=\mathbb{E}_u[\ln p(\mathbf{X}|\mu,\tau)+\ln p(\mu|\tau)]+\ln p(\tau)+const \\&amp;=(a_0-1)\ln \tau-b_o \tau+\frac{1}{2}\ln \tau+\frac{N}{2}\ln \tau \\&amp; -\frac{\tau}{2}\mathbb{E}_u[\sum^N_{n=1}(x_n-\mu)^2+\lambda_0(\mu-u_0)^2]+const \\\end{split}\]</span></p>
<p>对于<span class="math">\(\tau\)</span>,我们的先验分布有<span class="math">\(p(\tau)\)</span>与其他变量无关，所以在期望外面，而且对于整理后的形式，我们用可以把它化成gamma分布的形式，其参数为：</p>
<p><span class="math">\[\begin{split}a_N&amp;=a_0+\frac{N}{2} \\b_N&amp;=b_0+\frac{1}{2}\mathbb{E}_u[\sum^N_{n=1}(x_n-\mu)^2+\lambda_0(\mu-u_0)^2]\end{split}\]</span></p>
<p>然后为了求解出来具体的值，我们需要将两个式子解出来的参数根据分布的性质（这里指数分布家族的好处再次体现出来，充分统计量有着更良好的形式）联立，就能解出来，各个<span class="math">\(q\)</span>的参数的依赖关系。</p>
<p><span class="math">\[\begin{equation}\begin{split}&amp; u_N =\frac{\lambda_0u_0+N\bar{x}}{\lambda_0+N} \\&amp; \lambda_N =(\lambda_0+N)\frac{a_N}{b_N} \\&amp; a_N=a_0+\frac{N+1}{2}\\&amp;b_N=b_0+\frac{1}{2}[(\lambda_0+N)(\lambda^{-1}_N+\mu^2_N)\\ &amp;-2(\lambda_0u_0+\sum^N_{n=1}x_n)u_N+(\sum^N_{n=1}{x_n}^2)+\lambda_0{u_0}^2)]\\\end{split}\end{equation}\]</span></p>
<p>然后根据参数依赖迭代关系，迭代更新就好了。在这种方法中，我们也发现<span class="math">\(q\)</span>函数本身与<span class="math">\(p\)</span>也有着相同的形式，（直觉上是很自然的,至于内在的原因是不是与指数家族与共轭分布有关，还得多看看书），所以我们可以一开始就假设它满足这个形式，直接优化<span class="math">\(\mathcal{L}\)</span>。所以，我们直接写出来<span class="math">\(\mathcal{L}\)</span></p>
<p><span class="math">\[\begin{split}\mathcal{L} = &amp; \int q(Z) \ln\frac{p(X,Z)}{q(Z)}dZ \\ 
= &amp; E_q[\ln p(X,Z)] - E_q[\ln q(Z)] \\
= &amp; E_q[\ln p(X|\mu, \tau)] + E_q[\ln p(\mu|\tau)] + E_q[\ln p(\tau)] \\
&amp;- E[q_\mu(\mu)] - E[q_\tau[\tau]]
\end{split}\]</span></p>
<p>由于我们已经知道各个<span class="math">\(q_j\)</span>的分布，所以自然能写出来其似然期望与自身期望形式，即可以将<span class="math">\(\mathcal{L}\)</span>写作各个参数的形式，我们直接针对各个参数求偏导就可以求得迭代的形式了。这个地方的详细推倒可以在《Machine Learning:A Probabilistic Perspective》第21章里面找到。</p>
<p>到这里，变分推断的基础大致都看了一遍。也该回到我们LDA的问题上了。</p>
<p>回到LDA的生成过程上来，我们再来看一眼LDA的生成过程。</p>
<div class="figure">
<img src="http://nkssai.qiniudn.com/VBNP.png" alt="新生成过程"><p class="caption">新生成过程</p>
</div>
<p>我们发现其实整个模型中，<span class="math">\(\alpha\)</span>和<span class="math">\(\beta\)</span>都是超参，位置的参数只有<span class="math">\(\theta\)</span>，而<span class="math">\(z\)</span>为隐藏变量，我们难以优化的原因也是<span class="math">\(\theta\)</span>与<span class="math">\(z\)</span>纠缠在一起，所以我们干脆就把<span class="math">\(\theta\)</span>与<span class="math">\(z\)</span>的联系切断，即把<span class="math">\(\theta\)</span>和<span class="math">\(z\)</span>分到不同的组中，然后我们之前就提过我们对每一个<span class="math">\(q_j\)</span>的假设其实是不重要的，因为我们最终都是要将他们近似到<span class="math">\(p\)</span>上，所以可以在假设中加入参数，即可得到第二个图，切断了<span class="math">\(\theta\)</span>与<span class="math">\(z\)</span>的联系，然后又加入新的参数以作为对<span class="math">\(\theta\)</span>和<span class="math">\(z\)</span>的影响因子。</p>
<p>按照已经介绍的方法，我们可以写出<span class="math">\(\mathcal{L}\)</span>:</p>
<p><span class="math">\[\mathcal{L} = E_q[\ln p(w|z,\beta)] + E_q[\ln p(z|\theta)] + E_q[\ln p(\theta|\alpha)] - E_q[\ln q(\theta)] - E_q[\ln q(z)]\]</span></p>
<p>然后展开这个式子</p>
<p>对于<span class="math">\(E_q[\ln p(z|\theta)]\)</span>,首先有<span class="math">\(p(z|\theta)=\prod_{i=1}^{k}\theta_i^{(z_i)}\)</span>则其整个对数似然应该有<span class="math">\(\sum_n^N\sum_i^Kz_{in}\ln \theta_i\)</span>，对于同一文章的主题都有一个<span class="math">\(\theta\)</span>生成，则其期望有</p>
<p><span class="math">\[\begin{split}E_q[\ln p(z|\theta)] = &amp; E_q\big[\sum_{n=1}^N\sum_{i=1}^Kz_{in}\ln \theta_i\big] 
= E_q\big[\sum_{n=1}^N\ln \theta\sum_{i=1}^Kz_{in}\big] \\
= &amp; \sum_{n=1}^N\ln E_{q(\gamma)}[\ln\theta]\sum_{i=1}^KE_{q(\phi)}[z_{in}]
= \sum_{n=1}^N(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j))\sum_{i=1}^K\phi_{ni} \\
= &amp; \sum_{n=1}^N\sum_{i=1}^K\phi_{ni} (\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j))
\end{split}\]</span></p>
<p>这里注意其实<span class="math">\(z_{ni}\)</span>的期望就是<span class="math">\(\phi_{ni}\)</span>,最后Dirichlet分布的那个期望的证明在LDA原论文的附录A.1。 对于<span class="math">\(E_q[\ln q(z)]\)</span>，显然有</p>
<p><span class="math">\[\begin{split}E_{q(\phi)}[\ln q(z)]= &amp;E_{q(\phi)}\big[\ln \prod_{n=1}^N\prod_{i=1}^{k}\phi_{ni}^{z_{ni}}\big] \\ 
= &amp; E_{q(\phi)}\big[\sum_{n=1}^{N}\sum_{i=1}^{k}z_{ni}\ln \phi_{ni}\big]\\
= &amp; \sum_{n=1}^{N}\sum_{i=1}^{k}E[z_{ni}]\ln \phi_{ni} \\
= &amp; \sum_{n=1}^{N}\sum_{i=1}^{k}\phi_{ni}\ln \phi_{ni}
\end{split}\]</span></p>
<p>对于<span class="math">\(E_q[\ln p(w|z, \beta)]\)</span>有</p>
<p><span class="math">\[\begin{split}E_q[\ln p(w|z, \beta)] = &amp; E_q[\ln \prod_{i=1}^k\prod_{n=1}^N\prod_{j=1}^V\beta^{z_{ni}w_n^j}_{ij}] \\
= &amp; \sum_{n=1}^N\sum_{i=1}^k\sum_{j=1}^V\phi_{nj}w_n^j\ln \beta_{ij}
\end{split}\]</span></p>
<p>对于<span class="math">\(E_q[\ln q(\theta)]\)</span>有</p>
<p><span class="math">\[\begin{split} E_q[\ln q(\theta)] = &amp; (\sum_{j=1}^k(\gamma_j - 1)E_{q(\gamma)}[\ln \theta_i]) + \ln \Gamma(\sum_{i=1}^k\gamma_i)-\sum_{i=1}^k \ln\Gamma(\gamma_i) \\
= &amp; \sum_{i=1}^k(\gamma_i - 1)(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) + \ln \Gamma(\sum_{j=1}^k\gamma_j)-\sum_{i=1}^k \ln\Gamma(\gamma_i)
\end{split}\]</span></p>
<p>这里要注意的的是<span class="math">\(w_n\)</span>本身是一个长度为所有word个数的且其中只有一个元素为1其余为0的向量,而<span class="math">\(w_n^j\)</span>为文章第N个词是单词表第j个词的情况。所以，<span class="math">\(\beta_{ij}\)</span>是否有效，需要满足第n个词的主题是不是i即<span class="math">\(z_{ni}\)</span>,以及第N个词是不是单词表的第j个词即<span class="math">\(w_n^j\)</span>两个条件。</p>
<p>对于<span class="math">\(E_q[\ln p(\theta|\alpha)]\)</span>的计算与<span class="math">\(E_q[\ln q(\theta)]\)</span>相似两次展开Dirichlet分布就好了。</p>
<p><span class="math">\[E_q[\ln p(\theta|\alpha)]= \sum_{i=1}^k(\alpha_i - 1)(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) + \ln \Gamma(\sum_{i=1}^k\alpha_i)-\sum_{i=1}^k \ln\Gamma(\alpha_i)\]</span></p>
<p>则整个<span class="math">\(\mathcal{L}\)</span>为：</p>
<p><span class="math">\[\begin{split}
\mathcal{L}(\gamma,\phi;\alpha,\beta) = &amp; (\sum_{i=1}^k(\alpha_i - 1)(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) + \ln \Gamma(\sum_{i=1}^k\alpha_i)-\sum_{i=1}^k \ln\Gamma(\alpha_i) \\
+ &amp; \sum_{n=1}^N\sum_{i=1}^K\phi_{ni} (\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) \\
+ &amp; \sum_{n=1}^N\sum_{i=1}^k\sum_{j=1}^V\phi_{nj}w_n^j\ln \beta_{ij} \\
- &amp; \sum_{i=1}^k(\gamma_i - 1)(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) - \ln \Gamma(\sum_{j=1}^k\gamma_j)+\sum_{i=1}^k \ln\Gamma(\gamma_i) \\
- &amp; \sum_{n=1}^{N}\sum_{i=1}^{k}\phi_{ni}\ln \phi_{ni}
\end{split}\]</span></p>
<p>由于对于<span class="math">\(\phi\)</span>还有<span class="math">\(\sum_{i=1}^k\phi_{ni}=1\)</span>的约束，所以用拉格朗日乘子法,对于其中的一个<span class="math">\(\phi_{ni}\)</span></p>
<p><span class="math">\[\mathcal{L}_{[\phi_{ni]}}=\phi_{ni}(\psi(\gamma_i)-\psi(\sum_{j=i}^k\gamma_j))+\phi_{ni}\ln \beta_{iv}-\phi_{ni}\ln\beta_{iv}+\lambda_n(\sum_{j=1}^{k}\phi_{ni}-1)\]</span></p>
<p>这里<span class="math">\(\beta_{iv}\)</span>其实省略了之前的<span class="math">\(w_n^j\)</span>，对于每一个<span class="math">\(\phi_{ni}\)</span>都有对应的同一主题、同一单词的<span class="math">\(\beta\)</span>中的概率对应就是<span class="math">\(\beta_{iv}\)</span>。然后求偏导，令其等于零，显然有</p>
<p><span class="math">\[\phi_{ni} \propto \beta_{iv}exp(\psi(\gamma_i)-\psi(\sum_{j=1}^{k}\gamma_j))\]</span></p>
<p>再看<span class="math">\(\gamma_i\)</span></p>
<p><span class="math">\[$$\begin{split}
\mathcal{L}_{\gamma} = &amp; (\sum_{i=1}^k(\alpha_i - 1)(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) 
+ \sum_{n=1}^N\phi_{ni} (\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) \\
- &amp; \sum_{j=1}^k(\gamma_j - 1)(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j)) - \ln \Gamma(\sum_{j=1}^k\gamma_j)+\ln\Gamma(\gamma_i) \\
\end{split}\]</span>$$</p>
<p>即</p>
<p><span class="math">\[\mathcal{L}_{[\gamma]}=\sum_{i=1}^{k}(\psi(\gamma_i)-\psi(\sum_{j=1}^k\gamma_j))(\alpha_i+\sum_{n=1}^N\phi_{ni}-\gamma_i)-\ln \Gamma(\sum_{j=1}^{k}\gamma_j)+\ln \Gamma(\gamma_i)\]</span></p>
<p>对<span class="math">\(\gamma_i\)</span>求偏导，对于第一项，<span class="math">\(\sum_{i=1}^k\psi(\gamma_i)(\alpha_i+\sum_{n=1}^N\phi_{ni}-\gamma_i)\)</span>由于其中只有一个<span class="math">\(\gamma_i\)</span>，所以偏导应该为<span class="math">\(\psi&#39;(\gamma_i)(\alpha_i+\sum_{n=1}^N\phi_{ni}-\gamma_i) - \psi(\gamma_i)\)</span>,但对于<span class="math">\(\sum_{i=1}^k\psi(\sum_{j=1}^k\gamma_j)(\alpha_i+\sum_{n=1}^N\phi_{ni}-\gamma_i)\)</span>，这里由于对于每个i，都有<span class="math">\(\psi(\sum_{j=1}^k\gamma_j)\)</span>中存在一个<span class="math">\(\gamma_i\)</span>，故，其偏导为<span class="math">\(\psi&#39;(\sum_{j=1}^k\gamma_j)\sum_{j=1}^k(\alpha_j+\sum_{n=1}^N\phi_{nj}-\gamma_j)-\psi(\sum_{j=1}^{k}\gamma_j)\)</span>)，由于有<span class="math">\(\psi(a)=\frac{d}{da}\ln \Gamma(a)\)</span>,所以最后只剩下</p>
<p><span class="math">\[\frac{\partial \mathcal{L}}{\partial \gamma_i} = \psi&#39;(\gamma_i)(\alpha_i+\sum_{n=1}^N\phi_{ni}-\gamma_i)-\psi&#39;(\sum_{j=1}^k\gamma_j)\sum_{j=1}^k(\alpha_j+\sum_{n=1}^N\phi_{nj}-\gamma_j)\]</span></p>
<p>让<span class="math">\(\gamma_i\)</span>取得极大值的条件为</p>
<p><span class="math">\[\gamma_i=\alpha_i+\sum_{n=1}^{N}\phi_{ni}\]</span></p>
<p>到这里，我们得到了<span class="math">\(\gamma\)</span>与<span class="math">\(\phi\)</span>的迭代式子。下面应该找<span class="math">\(\alpha\)</span>与<span class="math">\(\beta\)</span>，我们发现我们在估计<span class="math">\(\gamma\)</span>和<span class="math">\(\beta\)</span>时，只是在一篇文章的层面上来观察模型，这我们也可以从生成过程图上理解这个问题，由于每篇文章都是可交换与独立同分布的所以，整个文集的极大似然，其实就是所有似然的累加，加上对于同一主题<span class="math">\(\beta_i\)</span>所有元素之和应该为一，在故对于<span class="math">\(\beta\)</span>有</p>
<p><span class="math">\[\mathcal{L}=\sum_{d=1}^{M}\sum_{n=1}^{N_d}\sum_{i=1}^{k}\sum_{j=1}^{V}\phi_{dni}w_{dn}^j\ln \beta_{ij}+\sum_{i=1}^k\lambda_i(\sum_{i=1}^k\beta_{ij}-1)\]</span></p>
<p>取偏导为零为</p>
<p><span class="math">\[\beta_{ij}\propto \sum_{d=1}^M\sum_{n=1}^{N_d}\phi_{dni}w_{dn}^j\]</span></p>
<p>对于<span class="math">\(\alpha\)</span>有</p>
<p><span class="math">\[\mathcal{L}_{[\alpha_i]}=\sum_{d=1}^M\big(\ln \Gamma(\sum_{j=1}^k\alpha_j)-\sum_{i=1}^k\ln \Gamma(\alpha_i)+\sum_{i=1}^{k}((\alpha_i-1)(\psi(\gamma_{di})-\psi(\sum_{j=1}^k\gamma_{dj}))\big)\]</span></p>
<p>由于每个文本中都有<span class="math">\(\alpha_i\)</span>，所以对于一个<span class="math">\(\alpha_i\)</span></p>
<p><span class="math">\[\frac{\partial\mathcal{L}}{\partial \alpha}=M(\psi(\sum_{j=1}^{k}\alpha_j)-\psi(\alpha_i))+\sum_{d=1}^{M}(\psi(\gamma_{di}-\psi(\sum_{j}^{k}\gamma_{dj})))\]</span></p>
<p>这个式子中要求对每一个<span class="math">\(\alpha_i\)</span>迭代的时候还需要依赖于其他的<span class="math">\(\alpha_{j\neq i}\)</span>，为了简化运算复杂度，采用LDA论文附录A.2中的方法,其Hessian的元素为</p>
<p><span class="math">\[\frac{\partial\mathcal{L}}{\partial \alpha_i \alpha_j}=\delta(i,j)M(\psi&#39;(\alpha_i)-\psi&#39;(\sum_{j=1}^k\alpha_j)\]</span></p>
<p>对于<span class="math">\(\alpha\)</span>的更新</p>
<p><span class="math">\[\alpha_{new} = \alpha_{old} - H(\alpha_{old})^{-1}g(\alpha_{old})\]</span></p>
<p>至此，对于LDA中所有的参数估计的过程。整个迭代过程为</p>
<p>整个过程也分为EM两步<br>E步为<br>  初始化所有的<span class="math">\(\phi_{ni}^0\)</span>为<span class="math">\(1/k\)</span><br>  初始化所有的<span class="math">\(\gamma_i\)</span>为<span class="math">\(\alpha_i+N/k\)</span><br>  repeat<br>    for n = 1 to N<br>      for i = 1 to k<br>        <span class="math">\(\phi_{ni}^{t+1}=\beta_{iw_n}exp(\psi(\gamma_i&#39;))\)</span><br>      归一化<span class="math">\(\phi_n^{t+1}\)</span><br>    <span class="math">\(\gamma^{t+1}=\alpha+\sum_{n=1}^N\phi_n^{t+1}\)</span> 这里都是向量</p>
<p>M步为<br> 对于每一个<span class="math">\(\beta_{ij} \propto \sum_{d=1}^M\sum_{n=1}^{N_d}\phi_{dni}^*w_{dn}^j\)</span><br> 对于<span class="math">\(\alpha\)</span>有<span class="math">\(\alpha_{new} = \alpha_{old} - H(\alpha_{old})^{-1}g(\alpha_{old})\)</span></p>

        </section>
        <hr/>
        <nav class="pagination" style="width:auto" role="pagination">
            
            <a data-pjax class="newer-posts" href="/2014/09/30/Gibbs-Sampling-and-B-LDA/">← Prev Post</a>
            
            <a class="share-button" data-original-title title>Share this Post</a>
            
            <a data-pjax class="older-posts" href="/2014/09/03/EM-and-GLAD/">Next Post →</a>
            
        </nav>
        <br/>
        <br/>
        <section id="comment">
            <div id="comment-box"></div>
        </section>


    </article>
</main>


  
<footer class="site-footer">
    
    <div class="inner">
        <section class="copyright"><a href="/"></a> &copy; 今天拒绝负能量 2014</section>
        <section class="poweredby">Published with <a target="_blank" href="http://hexo.io/">Hexo   </a> and Theme by <a target="_blank" href="https://github.com/yuche/hexo-theme-kael">Kael</a></section>
    </div>
</footer>
</div>
</div><!-- /scroller -->

</div><!-- /pusher -->
</div><!-- /container -->
</div>

<!-- Easter eggs -->

<div class="egg animated">
    <a id="close-button" href="#">X</a>
    <div class="block">
        <div class="loading">
            <span class="ball1"></span>
            <span class="ball2"></span>
        </div>
    </div>
</div>
  
<script src="//cdn.staticfile.org/jquery/1.11.0/jquery.min.js"></script>
<script>
    if (!window.jQuery) {
        var script = document.createElement('script');
        script.src = "/js/jquery.min.js";
        document.body.appendChild(script);
    }
</script>
<script type="text/javascript" src="/js/lib.js"></script>
<script type="text/javascript" src="/js/main.js"></script>





<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/x-mathjax-config">
$(document).on('pjax:end', function(){
MathJax.Hub.Typeset();
})
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</body>
</html>
